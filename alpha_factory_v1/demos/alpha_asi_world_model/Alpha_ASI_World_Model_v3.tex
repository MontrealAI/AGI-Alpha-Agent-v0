% ─────────────────────────────────────────────────────────────────────────────
% Alpha-Factory v1 – Multi-Agent AGENTIC α-AGI World-Model Demo (Research Paper)
% Compile:  xelatex Alpha_ASI_World_Model.tex  (needs TeX-Live ≥ 2023)
% ─────────────────────────────────────────────────────────────────────────────
\documentclass[11pt]{article}

%─── Fonts & Unicode ---
\usepackage[margin=1in]{geometry}
\usepackage{fontspec}
\setmainfont{Latin Modern Roman}
\newfontfamily\emoji{Noto Color Emoji}[Renderer=Harfbuzz] % fallbacks automatically if absent

%─── Hyperlinks, colours, tables, maths, graphics, lists ---
\usepackage{hyperref}
\hypersetup{colorlinks,allcolors=RoyalBlue}
\usepackage{graphicx,xcolor,tikz,amsmath,amsfonts,amssymb,booktabs,multirow,enumitem}
\usepackage{caption}
\captionsetup{labelfont=bf}

\title{\bfseries Alpha-Factory v1:\\
  Multi-Agent AGENTIC \boldmath$\alpha$-AGI World-Model Demo
  \texorpdfstring{\emoji{👁️}\,\emoji{✨}}{}}
\author{\textbf{MONTREAL.AI — AGI-Alpha-Agent-v0 Extension Team}}
\date{\today}

\begin{document}\maketitle
\begin{abstract}\noindent
We release a production-grade demonstration of \emph{Alpha-Factory v1}, an
antifragile multi-agent architecture that autonomously generates an
open-ended curriculum of synthetic worlds, trains generalist agents via
MuZero-style planning, and perpetually co-evolves both tasks and solvers with a
POET outer-loop.  Leveraging the OpenAI Agents SDK, Google ADK, the open
\textsc{A2A} protocol, and Anthropic’s Model Context Protocol, the system
integrates \emph{at least five} concrete Alpha-Factory agents to
\emph{Outlearn · Outthink · Outdesign · Outstrategise · Outexecute} across
industries—laying a practical foundation for the emergence of $\alpha$-ASI.
A REST/CLI/UI toolkit, Docker/Helm assets, and hardened safety guards make the
demo instantly deployable by non-technical stakeholders.
\end{abstract}

\tableofcontents
\newpage

%──────────────────────────────────────────────────────────────────────────────
\section{Introduction \& Objectives}\label{sec:intro}

The \textbf{Alpha-Factory v1 \emoji{👁️}\,\emoji{✨}} demo showcases a
\emph{large-scale foundation world-model} driven by a constellation of
autonomous agents (Fig.~\ref{fig:arch}).  Our objectives are:
\begin{enumerate}[leftmargin=2em,label=\textbf{O\arabic*.}]
  \item Generate a \emph{divergent stream of synthetic environments} whose
        novelty and difficulty scale with agent capability.
  \item Train a single \emph{generalist policy} able to transfer across tasks
        via MuZero-style model-based RL.
  \item Sustain \emph{open-ended co-evolution} of problems and solvers (POET).
  \item Guarantee \emph{secure, interpretable, regulator-ready} deployment.
  \item Provide \emph{push-button reproducibility}: one Docker/Helm command.
\end{enumerate}

%──────────────────────────────────────────────────────────────────────────────
\section{System Architecture}\label{sec:arch}

\begin{figure}[ht]\centering
\begin{tikzpicture}[node distance=3.1cm, every node/.style={draw,rounded
corners,align=center,font=\footnotesize,minimum width=2.5cm}]
\node (orch)  {Orchestrator\\(Macro-Sentinel)};
\node (env)   [above left of=orch] {Environment\\Generator};
\node (learner)[above right of=orch]{MuZero Learner};
\node (curr)  [below left of=orch] {Curriculum\\\& Eval};
\node (safety)[below right of=orch]{Safety Guardian};
\node (mem)   [below of=orch]      {Knowledge\\Memory};

\draw[->,thick] (env) -- (orch) -- (learner);
\draw[->,thick] (learner) edge[bend left] (env);
\draw[->,thick] (curr) -- (orch);
\draw[->,thick] (safety) -- (orch);
\draw[->,thick] (mem) -- (learner);
\end{tikzpicture}
\caption{High-level data/control flow (communications via \textsc{A2A}
message bus).}
\label{fig:arch}
\end{figure}

%──────────────────────────────────────────────────────────────────────────────
\subsection{Concrete Alpha-Factory Agents}

\begin{table}[ht]\centering
\caption{Five+ agents integrated in the demo and their cross-industry
contribution to the α-value pipeline.}
\label{tab:agents}
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Agent} & \textbf{Repository Class} & \textbf{Key Role in α-Value Creation}\\
\midrule
PlanningAgent & \texttt{planning\_agent.py} &
LLM-assisted decomposition of strategic objectives into solvable sub-tasks.\\
ResearchAgent & \texttt{research\_agent.py} &
Harvests \& filters external corpora/logs via MCP, enriching priors.\\
StrategyAgent & \texttt{strategy\_agent.py} &
Meta-gradient hyper-parameter tuning; drives antifragile adaptation.\\
MarketAnalysisAgent & \texttt{market\_agent.py} &
Synthesises live market feeds into domain-specific simulated worlds.\\
CodeGenAgent & \texttt{codegen\_agent.py} &
Secure self-modification of code/envs under SafetyAgent sandbox.\\
SafetyAgent\textsuperscript{†} & \texttt{safety\_agent.py} &
Alignment, policy-shaping, seccomp isolation (see §\ref{sec:safety}).\\
MemoryAgent\textsuperscript{†} & \texttt{memory\_agent.py} &
Vector + symbolic long-term memory surfaces via MCP.\\
\bottomrule
\end{tabular}

\vspace{0.4em}
\footnotesize\textsuperscript{†}\,Auxiliary agents exceeding the required
“5 minimum” but included for completeness.
\end{table}

%──────────────────────────────────────────────────────────────────────────────
\section{Learning Core}

\subsection{MuZero-Style Inner Loop}

Let $\mathcal{D} = \{(s_0,a_0,r_1,\dots,s_T)\}$ be the replay buffer.  
The network $\mathbf{f}_\theta = (\mathrm{repr},\mathrm{dyn},\mathrm{pred})$
optimises:
\begin{align}
\mathcal{L}(\theta) &=
\sum_{(s_0,\dots,s_T)\in\mathcal{D}}
\sum_{t=0}^{T}
\Bigl[
  (z_t - v_t)^2
  -\pi_t^\top \!\log p_t
  + \lambda_r (r_t - \hat r_t)^2
\Bigr]
+ \beta \lVert\theta\rVert_2^2 ,
\end{align}
where $z_t$ is the $n$-step return.  We employ
\emph{prioritized experience replay} with $\alpha=0.6$ and a
Dirichlet exploration prior $\mathcal{Dir}(0.3)$ on the root node.

\subsection{Generalisation Bound (sketch)}

Assuming Lipschitz continuity of $\mathrm{dyn}$ and polynomial
optimism in planning depth $d$, we obtain with probability $1-\delta$:
\[
  \bigl| J^\ast - J_{\text{demo}} \bigr|
  \le
  \tilde{\mathcal{O}}\!\left(
    \sqrt{\frac{d\,\log|\mathcal{S}|}{|\mathcal{D}|}}
    + \frac{\kappa}{\sqrt{m}}
  \right),
\]
where $J_{\text{demo}}$ is the empirical return, $m$ the number of
environments, and $\kappa$ the POET transfer constant.
Derivation follows Bartlett \& Mendelson (2002) Rademacher complexity bounds.

\subsection{Quality-Diversity Metric}

Environment novelty:
\[
\mathrm{QD}(e) = \alpha\,\text{novelty}(e)
               + (1-\alpha)\,\text{learning-gain}(e),
\]
with $\alpha\!=\!0.55$ empirically balancing exploration vs.\ utility.

%──────────────────────────────────────────────────────────────────────────────
\section{POET-Style Outer Loop}

\begin{figure}[ht]\centering
\fbox{\parbox{0.92\linewidth}{
\textbf{Algorithm 1:} \emph{Open-Ended Task/Agent Co-Evolution}
\begin{enumerate}[leftmargin=1.7em,label=(\arabic*)]
  \item Initialise $\mathcal{E}_0$ with base environment(s) $e_0$ and
        policy $\pi_{0}$.
  \item \textbf{Inner Loop:} Train $\pi_i$ on each $e_i\in\mathcal{E}_i$
        for $N$ steps using MuZero (§3).
  \item For each $e_i$, generate mutations $\{\tilde e\}$.
        Retain $\tilde e$ if $\mathrm{QD}(\tilde e)\!>\!\tau$ and
        minimal-criterion solvable.
  \item Attempt policy transfer
        $\pi_{i} \rightarrow \tilde e$; if success $>\eta$, keep pair
        $(\tilde e,\pi_{\tilde e})$.
  \item Set $\mathcal{E}_{i+1} = \mathcal{E}_i \cup \{\tilde e\}$,
        increment $i$, and repeat indefinitely.
\end{enumerate}}}
\caption{The outer loop drives perpetual innovation; see code §4.2.}
\end{figure}

%──────────────────────────────────────────────────────────────────────────────
\section{Safety \& Alignment}\label{sec:safety}

SafetyAgent enforces a KL-regularised prior
\[
\mathcal{L}_{\text{saf}}
  = \gamma\,\mathrm{KL}\!\bigl(
      \pi_\theta(\cdot\!\mid\!s)
      \big\|\,
      \pi_{\textit{safe}}(\cdot\!\mid\!s)
    \bigr),
\]
with a constitutional reference policy $\pi_{\textit{safe}}$
(LLM-distilled from vetted rules).  All CodeGen output is executed inside a
seccomp-\textsc{bpf} sandbox; network egress is disabled unless explicitly whitelisted.

%──────────────────────────────────────────────────────────────────────────────
\section{Deployment Pathways}

\paragraph{One-liner demo.}
\begin{verbatim}
docker run -p 7860:7860 ghcr.io/montrealai/alpha-asi-demo:latest
\end{verbatim}

\paragraph{Kubernetes.}
\verb|helm install alpha-asi ./helm_chart| installs every agent as a pod, with
GPU/CPU affinity and horizontal-pod-autoscaler YAML provided.

A fallback path disables all OpenAI-dependent components
(\texttt{OPENAI\_API\_KEY=""}) to guarantee offline operation.

%──────────────────────────────────────────────────────────────────────────────
\section{Conclusion}

\noindent
Alpha-Factory v1 operationalises an endlessly expanding \emph{era of
experience}.  By fusing open-ended environment generation, MuZero planning,
and rigorous safety scaffolding under a modular multi-agent umbrella, it
provides a tangible stepping stone toward $\alpha$-ASI.  Source code, Docker
images, Helm charts, CI badges, compliance matrices, and performance-tuning
playbooks reside in the accompanying GitHub directory.

\medskip\noindent
\textbf{Reproducibility Checklist ✓}\,: code + data + configs + CI badges.

\end{document}
