{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "colab": {
   "name": "AI-GA_Meta-Evolution_Demo.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": { "name": "python" }
 },
 "cells": [
  { "cell_type": "markdown",
    "source": [
      "# 🧬 AI-GA Meta-Evolution · Colab v3.4\n",
      "\n",
      "**Launch the complete three-pillar demo** (Clune 2020) in ~2 min — straight from your browser.\n",
      "\n",
      | feature | note |\n",
      |---------|------|\n",
      | Minimal deps | `torch`, `gymnasium`, `openai_agents`, `gradio`, `ray` (GPU optional) |\n",
      | LLM mode  | `OPENAI_API_KEY` → online; else Mixtral via Ollama cloud binary |\n",
      | Dashboards | Gradio (7862) + FastAPI (8000 → `/docs`) |\n",
      | Tests      | `pytest -q` ≥ 90 % branch cov. < 0.5 s |\n",
      | Observability | Prometheus scrape, fitness plot cell |\n",
      | Scale-out  | Ray remote eval; K8s YAML snippet |\n",
      | SOC-2 | no secrets on disk, signed image pull example |\n"
    ]
  },

  { "cell_type": "code",
    "metadata": { "id": "00_gpu_flag" },
    "source": [
      "#@title ↳ Select runtime   { run: \"auto\", display-mode: \"form\" }\n",
      "USE_GPU = False  #@param {type:\"boolean\"}\n",
      "import os; os.environ[\"USE_GPU\"] = str(USE_GPU)\n",
      "print(\"⚙️ GPU\" if USE_GPU else \"⚙️ CPU\", \"runtime selected\")"
    ]
  },

  { "cell_type": "code",
    "metadata": { "id": "01_setup" },
    "source": [
      "%%bash --no-stderr\n",
      "set -Eeuo pipefail\n",
      "REPO=AGI-Alpha-Agent-v0\n",
      "[[ -d $REPO ]] || git clone --depth 1 https://github.com/MontrealAI/$REPO.git -q\n",
      "cd $REPO/alpha_factory_v1/demos/aiga_meta_evolution\n",
      "\n",
      "# choose proper torch wheel index\n",
      "IDX=$(python - <<PY\n",
      "import os, sys; print('cu118' if os.getenv('USE_GPU')=='True' else 'cpu')\n",
      "PY)\n",
      "pip -q install --upgrade pip\n",
      "pip -q install torch torchvision --extra-index-url https://download.pytorch.org/whl/$IDX\n",
      "pip -q install gymnasium[classic_control] gradio==4.* openai_agents ray httpx prometheus-client pytest coverage\n",
      "echo '✅ Dependencies installed'"
    ]
  },

  { "cell_type": "code",
    "metadata": { "id": "02_key" },
    "source": [
      "import os, getpass\n",
      "if not os.getenv(\"OPENAI_API_KEY\"):\n",
      "    key = getpass.getpass(\"Paste OPENAI_API_KEY (leave empty for offline mode): \")\n",
      "    if key:\n",
      "        %env OPENAI_API_KEY=$key\n",
      "        print(\"🔑 Key set — online mode enabled\")\n",
      "    else:\n",
      "        print(\"🛰️ Offline mode — Mixtral/Ollama fallback\")"
    ]
  },

  { "cell_type": "code",
    "metadata": { "id": "03_tests" },
    "source": [
      "%%bash\n",
      "cd AGI-Alpha-Agent-v0/alpha_factory_v1/demos/aiga_meta_evolution\n",
      "pytest -q --disable-warnings || echo '⚠️ Some tests failed (demo still runnable)'"
    ]
  },

  { "cell_type": "code",
    "metadata": { "id": "04_launch" },
    "source": [
      "import subprocess, threading, re, time, pathlib, sys, os\n",
      "ROOT = pathlib.Path('AGI-Alpha-Agent-v0/alpha_factory_v1/demos/aiga_meta_evolution').resolve()\n",
      "os.chdir(ROOT)\n",
      "proc = subprocess.Popen([sys.executable, 'agent_aiga_entrypoint.py'],\n",
      "                        stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
      "                        text=True)\n",
      "public = None\n",
      "def _pipe():\n",
      "  global public\n",
      "  for ln in proc.stdout:\n",
      "    print(ln, end='')\n",
      "    if not public and 'Running on' in ln and 'https' in ln:\n",
      "      public = re.search(r'https?://[\\w./-]+', ln)[0]\n",
      "      print(f'\\n🔗 Gradio dashboard → {public}\\n')\n",
      "threading.Thread(target=_pipe, daemon=True).start()\n",
      "for _ in range(90):\n",
      "  if public: break\n",
      "  time.sleep(1)\n",
      "if not public: print('⏳ Still starting … check logs above')"
    ]
  },

  { "cell_type": "markdown", "source": "## ☎️ FastAPI helper" },

  { "cell_type": "code",
    "metadata": { "id": "05_api" },
    "source": [
      "import httpx, json\n",
      "API='http://localhost:8000'\n",
      "print('Health →', httpx.get(API+'/health').json())\n",
      "print('Schedule 10 generations'); httpx.post(API+'/evolve/10')"
    ]
  },

  { "cell_type": "markdown", "source": "## 📈 Prometheus scrape" },

  { "cell_type": "code",
    "metadata": { "id": "06_metrics" },
    "source": [
      "import re, httpx, time\n",
      "raw=httpx.get('http://localhost:8000/metrics').text\n",
      "gen=int(re.search(r'aiga_generations_total (\\d+)',raw).group(1))\n",
      "fit=float(re.search(r'aiga_best_fitness (\\d+\\.?\\d*)',raw).group(1))\n",
      "print(f'Gen {gen}, best fitness {fit:.2f}')"
    ]
  },

  { "cell_type": "markdown", "source": "## 📊 Plot history" },

  { "cell_type": "code",
    "metadata": { "id": "07_plot" },
    "source": [
      "import glob, json, pandas as pd, matplotlib.pyplot as plt, pathlib\n",
      "ckpt=sorted(glob.glob('checkpoints/evolver_gen*.json'))[-1]\n",
      "hist=json.loads(pathlib.Path(ckpt).read_text())[\"history\"]\n",
      "pd.DataFrame(hist,columns=['gen','avg']).plot(x='gen',y='avg',grid=True,figsize=(6,3));"
    ]
  },

  { "cell_type": "markdown",
    "source": [
      "---\n",
      "## 🏗 Kubernetes snippet\n",
      "```yaml\n",
      "apiVersion: apps/v1\n",
      "kind: Deployment\n",
      "metadata: { name: aiga-demo }\n",
      "spec:\n",
      "  replicas: 1\n",
      "  selector: { matchLabels: { app: aiga-demo } }\n",
      "  template:\n",
      "    metadata: { labels: { app: aiga-demo } }\n",
      "    spec:\n",
      "      containers:\n",
      "      - name: orchestrator\n",
      "        image: ghcr.io/montrealai/alpha-aiga:latest@sha256:<signed>\n",
      "        envFrom: [{ secretRef: { name: aiga-secrets } }]\n",
      "        ports:\n",
      "        - { containerPort: 8000 }\n",
      "        - { containerPort: 7862 }\n",
      "```"
    ]
  },

  { "cell_type": "markdown",
    "source": "### 🎯 Next steps — tweak `meta_evolver.py`, add curriculum stages, or integrate the REST API into your own autonomous pipelines." }
 ]
}
